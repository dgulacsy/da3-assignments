---
title: "Finding fast growing firms"
subtitle: "DA3 Assignment 2"
author: "Attila Szuts, Dominik Gulacsy"
date: "2/10/2021"
code_download: yes
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

## [Github Repo](https://github.com/dgulacsy/da3-assignments/tree/main/da3-assignment2)

```{r}
rm(list = ls())
library(tidyverse)
library(caret)
library(party)
library(GGally)
library(pROC)

source("codes/helper.R")
```

# Introduction
In this analytics project we are aiming to find the best model to predict fast growth firms based on their various characteristics and financial indicators. To place this task into a business environment we assumed that the main business motivation behind this whole analytics project is to identify those firms that worth investing in. We did not define a specific form of investment worth considering the predicted firms may be considered for fusion, acquisition or simple buy-in. To find the best a model, first of all we go through all the steps of sample design, label engineering and feature engineering. We also look at some results of our exploratory data analysis to see which particular aspects of the data we should keep in mind and what kind of model specifications we should come up. During our analysis we run altogether 7 models increasing complexity and finally a model using LASSO. I compare and assess model performance and recommend a final model for the prediction problem.


# Label engineering



# Data Cleaning/Wrangling





# Model preparation

## Train test split

```{r}
df <- read_rds("data/clean/fast-growth-firms-workfile.rds")

# skim(df)

# create smaller sample to test models
samp_size <- round(nrow(df) / 20)
set.seed(1234)
df <- sample_n(df, samp_size)
nrow(df)
df <- df[!is.na(df$is_fg),]
nrow(df)
df <- na.omit(df)
nrow(df)

set.seed(1234)
training_ratio <- 0.7
train_indices <- createDataPartition(
  y = df[["is_fg"]],
  times = 1,
  p = training_ratio,
  list = FALSE
) %>% as.vector()
data_train <- df[train_indices, ]
data_test <- df[-train_indices, ]
```

## Variable selection

```{r}
target <- c("is_fg")
business <- c("ind2_cat","urban","region","labor_avg","age","age2","new")
ceo <- c("ceo_inoffice_years","ceo_age","ceo_count",
        "ceo_female","ceo_foreign","ceo_gender","ceo_origin")
sales <- c("sales_mil_log","d1_sales_mil_log")
financial_basic <- c("curr_assets","curr_liab","fixed_assets","tang_assets",
                     "intang_assets","inventories","liq_assets","subscribed_cap",
                     "share_eq","material_exp","personnel_exp","amort","profit","d1_profit")
financial_ext <- c("extra_exp","extra_inc","extra_profit_loss","inc_bef_tax")
financial_basic_ratios <- colnames(df %>% select(matches("*._bs|*._pl")))
financial_ext_ratios <- colnames(df %>% select(matches("*._ratio")))
flags <- colnames(df %>% select(matches("*.flag.")))

formula_lpm = formula(paste0(target, paste0(" ~ ",paste(c(business, ceo,sales, financial_basic, financial_ext, financial_basic_ratios, flags),collapse = " + "))))
formula_log = formula(paste0(target, paste0(" ~ ",paste(c(business, ceo,sales, financial_basic, financial_ext, financial_basic_ratios, flags),collapse = " + ")))) 
formula_rf = formula(paste0(target, paste0(" ~ ",paste(c(business, financial_basic, flags),collapse = " + "))))  # ceo,sales, financial_ext, financial_basic_ratios, 
formula_gbm = formula(paste0(target, paste0(" ~ ",paste(c(business, financial_basic, flags),collapse = " + "))))  # ceo,sales, financial_ext, financial_basic_ratios, 
formula_knn = formula(paste0(target, paste0(" ~ ",paste(c(business, financial_basic, flags),collapse = " + "))))  # ceo,sales, financial_ext, financial_basic_ratios, 
formula_enet = formula(paste0(target, paste0(" ~ ",paste(c(business, financial_basic, flags),collapse = " + "))))  # ceo,sales, financial_ext, financial_basic_ratios, 
```

# EDA
```{r}

df %>% select(where(is.numeric)) %>% ggcorr(layout.exp = 1)
ggpairs(df, columns = c(target, business))

# Check for multicollinearity and perfect collinearity
numeric_df <- keep( df , is.numeric )
cT <- cor(numeric_df , use = "complete.obs")

# Check for highly correlated values:
sum( abs( cT ) >= 0.8 & cT != 1 ) / 2
# Find the correlations which are higher than 0.8
id_cr <- which( abs( cT ) >= 0.8 & cT != 1 )
pair_names <- expand.grid( variable.names(numeric_df) , variable.names(numeric_df) )
# Get the pairs:
high_corr <- pair_names[ id_cr , ]
high_corr <- mutate( high_corr , corr_val = cT[ id_cr ] )
high_corr <- mutate(high_corr, Var1 = as.character(Var1))
high_corr <- mutate(high_corr, Var2 = as.character(Var2))
high_corr$vars<-unlist(lapply(1:nrow(high_corr),function(i){
  paste(sort(c(high_corr$Var1[i],high_corr$Var2[i])), collapse = "")
}))
high_corr<-high_corr[!duplicated(high_corr$vars),]
high_corr

```

# Modelling

```{r}
### In progress -----------------

# 5 fold cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)


# Train Logit Models ----------------------------------------------

logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)

CV_RMSE_folds <- list()
logit_models <- list()

for (model_name in names(logit_model_vars)) {

  features <- logit_model_vars[[model_name]]

  set.seed(13505)
  glm_model <- train(
    formula(paste0("default_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )

  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]

}

# Logit lasso -----------------------------------------------------------

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(13505)
system.time({
  logit_lasso_model <- train(
    formula(paste0("default_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )
})

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
write.csv(lasso_coeffs, paste0(output, "lasso_logit_coeffs.csv"))

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]


#############################################x
# PART I
# No loss fn
########################################

# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {

  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)

    roc_obj <- roc(cv_fold$obs, cv_fold$default)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }

  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                              "AUC" = unlist(auc))
}

# For each model: average RMSE and average AUC for models ----------------------------------

nonCV_RMSE <- list()
CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}


```

# LPM

```{r}
model_lpm <- lm(formula_lpm, data = data_train)
model_lpm
```


# Logit

```{r}
model_logit <- glm(formula = formula_log, data = data_train, family = "binomial")
# 
# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred
# 
model_logit
```


# Probit

```{r}
model_probit <- glm(formula = formula_log, data = data_train, family = binomial(link = "probit"))
# 
# Warning message:
# glm.fit: fitted probabilities numerically 0 or 1 occurred 
# 
model_probit
```

# Random Forest

```{r}
trctrl <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
trctrl$verboseIter <- TRUE

start <- Sys.time()
# use all variables?
model_rf <- train(formula_rf,
                  data = data_train, 
                  method = "rf",
                  metric = "ROC",
                  trControl=trctrl,
                  num.threads = 7,
                 na.action = na.omit)

# 
# Error: Please use column names for `x`
# 

send_message()
model_caret
postResample(data_test$is_fg, predict(model_caret, data_test))

# colnames(data_test, 2)
print(end - start)
```

```{r}
## Alternative tuning
train_control <- trainControl(
  method = "cv",
  n = 3,
  classProbs = TRUE, # same as probability = TRUE in ranger
  # summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
train_control$verboseIter <- TRUE

tune_grid <- expand.grid(
  .mtry = c(5, 6, 7),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)

# getModelInfo("ranger")
set.seed(13505)
rf_model_p <- train(
  data_train[formula_rf], data_train[["is_fg"]],
  method = "ranger",
  data = data_train,
  tuneGrid = tune_grid,
  trControl = train_control
)

# 
# model fit failed for Fold5: mtry=7, splitrule=gini, min.node.size=15 Error in ranger::ranger(dependent.variable.name = ".outcome", data = x,  : 
#   formal argument "data" matched by multiple actual arguments
# 
  
```

# Boosting

```{r}
start_time <- Sys.time()
# Train model with preprocessing & repeated cv
trctrl <- trainControl(method = "repeatedcv", 
                       number = 3, 
                       repeats = 1, 
                       verboseIter = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)

model_gbm <- train(formula_gbm,
                   data = data_train,
                   method = "gbm",
                   trControl = trctrl,
                   verbose = 0,
                   # num.threads = 7,
                   na.action = na.omit)
end_time <- Sys.time()
model_gbm

# 
# Error: Please use column names for `x`
# 

print(end_time - start_time)
send_message()
```


# KNN

```{r}
set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number = 3, repeats = 1, verboseIter = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)


model_knn <- train(formula_knn, data = data_train, method = "knn",
                 trControl=trctrl,
                 preProcess=c("center", "scale"),
                 tuneLength=20,
                 # num.threads = 7
                 na.action = na.omit
                 #tuneGrid = data.frame(k=c(2:8))
                 )
model_knn

# 
# Error in roc.default(predictor = predict(model_knn, data_test, type = "prob",  : 
#  No valid data provided.
#

send_message()

lmElasticNetCaret
knnModelRoc <- roc(predictor = predict(model_knn, data_test, type='prob', decision.values=T)$is_fg, response = data_test$is_fg)
knnModelRoc
plot(knnModelRoc)

postResample(SpamTest$count, predict(model_knn, SpamTest))
```


# Elastic Net
```{r}
set.seed(1234)
trctrl <- trainControl(method = "cv", classProbs=TRUE, summaryFunction=twoClassSummary, verboseIter = TRUE, number = 3)

lmElasticNetCaret <- train(formula_enet, 
                 data = data_train, 
                 method = "glmnet",
                 trControl=trctrl,
                 tuneLength=5,
                 metric='ROC',
                 family = "binomial",
                 # num.threads = 7,
                 na.action = na.omit
                 #tuneGrid = data.frame(lambda=seq(0.1,1,0.1), alpha=rep(1, 10))
)

# 
# Error in colnames(data) : argument "data" is missing, with no default
# 

```


```{r}
lasso_tune_grid <- expand.grid(
  "alpha" = c(1),
  "lambda" = seq(0.05, 0.5, by = 0.025)
)

ridge_tune_grid <- expand.grid(
  "alpha" = c(0),
  "lambda" = seq(0.05, 0.5, by = 0.025)
)

enet_tune_grid <- expand.grid(
  "alpha" = seq(0, 1, by = 0.1),
  "lambda" = union(lasso_tune_grid[["lambda"]], ridge_tune_grid[["lambda"]])
)

fit_control <- trainControl(method = "cv", 
                            number = 3,
                            verboseIter = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary)
                            

set.seed(1234)
enet_fit <- train(
  formula_enet,
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = enet_tune_grid,
  family = "binomial",
  trControl = fit_control,
  na.action = na.omit
)

# 
# Error in na.fail.default(list(is_fg = c(2, 1, 1, 1, 1, 2, 2, 1, 2, 2,  : 
#   missing values in object
# 

enet_fit
```



